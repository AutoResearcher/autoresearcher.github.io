<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research | Physical Intelligence (π)</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Physical Intelligence (π)</h1>
            <nav>
                <ul>
                    <li><a href="index.html">Home</a></li>
                    <li><a href="#" class="active">Research</a></li>
                    <li><a href="join.html">Join Us</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <main>
        <section class="intro">
            <div class="container">
                <h2>Our Research</h2>
                <p>
                    At Physical Intelligence, we focus on developing general-purpose AI systems for 
                    robotic control and physical interaction. Our research combines reinforcement learning, 
                    computer vision, and intelligent control systems to create versatile robot policies.
                </p>
            </div>
        </section>

        <section class="research-areas">
            <div class="container">
                <div class="research-area">
                    <div class="dot"></div>
                    <h3>Generalist Robot Policies</h3>
                    <p>
                        We are developing flexible control systems that enable robots to perform 
                        a wide range of tasks in diverse environments. Our policies can transfer skills 
                        between different robots and adapt to new situations with minimal retraining.
                    </p>
                    <ul>
                        <li>Multi-task and multi-embodiment transfer learning</li>
                        <li>Open-world generalization for robotic manipulation</li>
                        <li>Large-scale robot learning from diverse data</li>
                    </ul>
                </div>

                <div class="research-area">
                    <div class="dot"></div>
                    <h3>Vision-Language-Action Models</h3>
                    <p>
                        Our VLA models integrate vision, language understanding, and action generation 
                        to allow robots to understand natural language instructions and visual cues 
                        when performing physical tasks.
                    </p>
                    <ul>
                        <li>Language-conditioned robot control</li>
                        <li>Visual scene understanding for manipulation</li>
                        <li>Cross-modal representations for physical tasks</li>
                    </ul>
                </div>

                <div class="research-area">
                    <div class="dot"></div>
                    <h3>Embodied Intelligence</h3>
                    <p>
                        We're studying how physical embodiment affects learning and adaptation, creating 
                        systems that understand and leverage the physical properties of robots and their 
                        environments.
                    </p>
                    <ul>
                        <li>Physically-grounded reasoning</li>
                        <li>Dexterous manipulation capabilities</li>
                        <li>Environmental adaptation and robustness</li>
                    </ul>
                </div>

                <div class="research-area">
                    <div class="dot"></div>
                    <h3>Robot Learning Infrastructure</h3>
                    <p>
                        We develop systems and tools that enable efficient training, evaluation, and 
                        deployment of robot learning algorithms at scale across diverse hardware platforms.
                    </p>
                    <ul>
                        <li>Distributed robot learning frameworks</li>
                        <li>Efficient action tokenization and representation</li>
                        <li>Hardware-software interfaces for diverse robots</li>
                    </ul>
                </div>
            </div>
        </section>

        <section class="publications">
            <div class="container">
                <h2>Publications</h2>
                <div class="publication">
                    <div class="dot"></div>
                    <h3>π0.5: A Vision-Language-Action Model for Open-World Robot Generalization</h3>
                    <p class="authors">Black, K., Brown, N., Driess, D., et al.</p>
                    <p class="venue">Conference on Robot Learning (CoRL), 2025</p>
                    <p class="abstract">
                        We present π0.5, a model that extends our previous work to enable robots to 
                        generalize to entirely new environments and tasks through its integrated 
                        vision-language-action architecture.
                    </p>
                </div>

                <div class="publication">
                    <div class="dot"></div>
                    <h3>FAST: Tokenizing Robot Actions for Efficient Policy Learning</h3>
                    <p class="authors">Darpinian, J., Dhabalia, K., Esmail, A., et al.</p>
                    <p class="venue">Robotics: Science and Systems (RSS), 2024</p>
                    <p class="abstract">
                        We introduce FAST, a novel tokenization approach for robot action sequences 
                        that improves training efficiency by 5x compared to previous continuous 
                        representation methods.
                    </p>
                </div>

                <div class="publication">
                    <div class="dot"></div>
                    <h3>π0: A Generalist Policy for Robotic Manipulation</h3>
                    <p class="authors">Driess, D., Black, K., Brown, N., et al.</p>
                    <p class="venue">International Conference on Robotics and Automation (ICRA), 2024</p>
                    <p class="abstract">
                        We describe π0, our first generalist robot policy that demonstrates unprecedented 
                        flexibility across diverse manipulation tasks and robot embodiments through 
                        large-scale data collection and a novel network architecture.
                    </p>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 Physical Intelligence. All rights reserved.</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html> 